varImpPlot(rf.fit)
summary(y_pred)
summary(y_pred)
y_pred = predict(rf.fit, newdata = raw.test[,-1])
summary(y_pred)
summary(as_numeric(y_pred))
class(y_pred)
summary(y_pred)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = predict(rf.fit, newdata = test[,-10])
summary(y_pred)
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
table(y_pred, ValidSet$Condition)
table(y_pred, test$Class)
View(test)
y_pred
mean(y_pred == test$Class)
mean(y_pred == train$Class)
importance(rf.fit)
varImpPlot(rf.fit)
varImpPlot(rf.fit)
rf.fit <- randomForest(cc ~ ., data = raw.training, importance = T, type = "classification")
y_pred = predict(rf.fit, newdata = raw.test[,-1])
summary(y_pred)
rf.roc<-roc(raw.test$cc, as.numeric(y_pred))
plot(rf.roc)
rf.roc
View(boot.m)
test[,-10]
y_pred
y_pred = predict(rf.fit, newdata = test[,-10])
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = predict(rf.fit, newdata = test[,-10])
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = trunc(predict(rf.fit, newdata = test[,-10]))
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = predict(rf.fit, newdata = test[,-10])
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
y_pred = predict(rf.fit, newdata = test[,-10])
y_pred
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
table(y_pred, test$class)
table(y_pred, test$Class)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
table(y_pred, test$Class)
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
summary(y_pred)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
summary(y_pred)
y_pred
rf.fit <- randomForest(cc ~ ., data = raw.training, importance = T, type = "classification")
y_pred = predict(rf.fit, newdata = raw.test[,-1])
summary(y_pred)
rf.roc<-roc(raw.test$cc, as.numeric(y_pred))
plot(rf.roc)
rf.roc
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
summary(y_pred)
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
table(y_pred, test$Class)
# Area under the curve: 0.9815. We controlled the complexity by limiting the number of trees permitted.
importance(y_pred)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
importance(y_pred)
importance(rf.fit)
rf.fit <- randomForest(Class ~ ., data = train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit <- randomForest(Class ~ ., data = train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit <- randomForest(Class ~ train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
rf.fit <- randomForest(data$Class ~ train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
rf.fit <- randomForest(train$Class ~ train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit <- randomForest(train$Class ~ train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
importance(rf.fit) # Determine importance variables
summary(y_pred)
rf.roc<-roc(test$Class, y_pred) # Compute RO-curve
plot(rf.roc) # Plot the cuve
rf.roc
table(y_pred, test$Class)
# Area under the curve: 0.9815. We controlled the complexity by limiting the number of trees permitted.
# When looking at the importance of the variables we have looked at the %IncMSE. By looking at these percentages we determine all variables are important, even though some more then others.
pROC::roc.test(rf.roc, AUC.ORIGINAL - mean(AUCS.m - AUCS.orig), method = "bootstrap", boot.n = 500, progress = "none", paired = T)
rf.roc
rf.roc$auc
pROC::roc.test(rf.roc$auc, AUC.ORIGINAL - mean(AUCS.m - AUCS.orig), method = "bootstrap", boot.n = 500, progress = "none", paired = T)
pROC::roc.test(rf.roc$auc, AUC.ORIGINAL, method = "bootstrap", boot.n = 500, progress = "none", paired = T)
d<-rf.roc$auc
pROC::roc.test(d, AUC.ORIGINAL, method = "bootstrap", boot.n = 500, progress = "none", paired = T)
pROC::roc.test(d, AUC.ORIGINAL,  boot.n = 500, progress = "none", paired = T)
pROC::roc.test(d, AUC.ORIGINAL, method = "delong",  boot.n = 500, progress = "none", paired = T)
library(tidyverse) # Includes many libraries like dplyr (for easy data maniputaion, readr (for reading datasets), ggplot2 (for creating elegant data visualisations), tibble (for easily working with data frames), etc.)
library(rms)       # for implementing Regression Modeling Strategies
library(ROCR)      # for visualizing the performance of prediction models
library(pROC)      # for Analyzing ROC curves
library(mlbench)   # for provision of machine learning benchmark problems
library(MASS)      # for stepwise variable selection
options(dplyr.print_min = 5L, dplyr.print_max = 5L) # set number of rows to show in a data frame
data(BreastCancer, package="mlbench")
names(BreastCancer) # * What are the names of the data frame bc?
nrow(BreastCancer)  # * How many obseravtion (rows) are there?
# Take a moment to look at the description of the database on http://ugrad.stat.ubc.ca/R/library/mlbench/html/BreastCancer.html
# Print the dataset
BreastCancer
summary(BreastCancer)
sum(is.na(BreastCancer$Bare.nuclei)) ## There are 16 missing values in the Bare.nuclei column.
# => Are there any missing values? There are 16 missing values in the Bare.nuclei column.
# => Write the code to calculate the number of missing values in each column. You can use whatever suits you. For example you can use "sapply" on BreastCancer to work on all columns while using the is.na function, or (preferably) you can use the power of dplyr commands like "summarise" together with sum and "is.na".
#
#
# => We will use only complete cases without any missing values in this excercise. Obtain the data frame "bc" with complete cases. Hint: look at the function complete.cases(). You can use it for example with the filter command of dplyr (or just use base R)
bc <- subset(BreastCancer, is.na(Bare.nuclei) == F)  # dataset with complete cases
# => How many obseravtion (rows) are there now?
# There are now 683 observations, because the 16 rows with missin g values in Bare.nuclei where deleted.
# How many cases did we remove?
# 16
# remove id column
bc <- subset(bc, select = -1) #=> Remove the "id" column from bc. Note that if you consider to use the "select" command in dplyr then this command may clash with the select command in the MASS library. Therefore use dplyr::select in that case.
# => convert the first 9 factors to numeric. You can use a for loop on these variables, or use "mutate_at".
bc$Cl.thickness <- as.numeric(bc$Cl.thickness)
bc$Cell.size <- as.numeric(bc$Cell.size)
bc$Cell.shape <- as.numeric(bc$Cell.shape)
bc$Marg.adhesion <- as.numeric(bc$Marg.adhesion)
bc$Epith.c.size <- as.numeric(bc$Epith.c.size)
bc$Bare.nuclei <-  as.numeric(bc$Bare.nuclei)
bc$Bl.cromatin <- as.numeric(bc$Bl.cromatin)
bc$Normal.nucleoli <- as.numeric(bc$Normal.nucleoli)
bc$Mitoses <- as.numeric(bc$Mitoses)
str(bc)
# Look at the class variable
bc$Class
# => change "malignant" into the number 1, and "bening" into the number 0. You can use a simple ifelse, or use the "recode" command in dplkr.
bc$Class <- ifelse(bc$Class == "malignant", 1, 0)
ddist <- datadist(bc) # preparation: in package rms we need to define the data distribution of the variables first. So before any models are fitted, this command stores the distribution summaries (e.g. ranges) for all potential variables.
options(datadist='ddist') # This means that when we fit a model we will also store the distribution information with the model object as well
mitoses.lrm <- lrm(Class~Mitoses, x=T, y=T, data=bc) # x = T and y=Y mean that the object mitoses.lrm will not only inlcude the model but will also keep the data as well. This is useful when we want access to the data via the model object.
summary(mitoses.lrm)
mitoses.thickness.lrm <- lrm(Class ~ Mitoses + Cl.thickness, x=T, y=T, data=bc) # fit a model that includes Mitoses and Cl.thickness as covariates
summary(mitoses.thickness.lrm)
set.seed(1234) # we fix the seed so that results are the same for all students
train.smp.size <- round(0.7 * nrow(bc))
test.smp.size <- round(0.3 * nrow(bc))
####roandom columns
train.ind <- sample(seq_len(nrow(bc)), size = train.smp.size)
train <-  bc[train.ind, ] # => Obrain train set consisting of 70% of the data
test <-  bc[-train.ind, ] #=> Obtain test set consisting of the rest of observations
mitoses.train.lrm <- lrm(Class ~ Mitoses, x=T, y=T, data=train) # => fit lrm model on the training set using only Mitoses, use x=T and y=T
mitoses.thickness.train.lrm <- lrm(Class ~ Mitoses + Cl.thickness, x=T, y=T, data=train) # => fit lrm model on the training set using  Mitoses and Cl.thickness, use x=T and y=T
predicted.mitoses.test <- predict(mitoses.train.lrm, newdata = test, type = 'fitted') # => obtain the predicted probabilities of mitoses.train.lrm on the test set. Hint: use the "predict" function. Important note: make sure you use the right "type" in the command to get probabilities and not log odds.
predicted.mitoses.thickness.test <- predict(mitoses.thickness.train.lrm, test, type = "fitted") # => obtain the predicted probabilities of mitoses.thickness.train.lrm on the test set.Check that they indeed are between 0 and 1 and have no negative numbers etc.
# => plot histogram of predicted.mitoses.test
hist(predicted.mitoses.test)
#* => plot histogram of predicted.mitoses.thickness.test
hist(predicted.mitoses.thickness.test)
#* => Obtain range of predicted.mitoses.test
summary(predicted.mitoses.test)
#* => Obtain range of predicted.mitoses.thickness.test
summary(predicted.mitoses.thickness.test)
mitoses.test.pred.class <- data.frame(pr=predicted.mitoses.test, cl = as.factor(test$Class))
ggplot(mitoses.test.pred.class, aes(pr, fill = cl)) + geom_density(adjust = 2, alpha = 0.5) + xlab("predicted probability")
# => For this density plot, which of the following statements are true:
# For those without breast cancer the probabilities are concentrated below 0.4 True
# For those with breast cancer the probabilities are concentrated above 0.8 False
# For those without breast cancer the probabilities are very high True
# For those with breast cancer they are likely to have any probability. True
# => What do you think that the "adjust" above does. Try the value 1 instead of 2. What does alpha do? Try alpha = 1
# We see that the color changes.
# => plot the probability density graph as before but now for the predicted.mitoses.thickness.test
mitoses.thick.test.pred.class <- data.frame(pr=predicted.mitoses.thickness.test, cl = as.factor(test$Class))
ggplot(mitoses.thick.test.pred.class, aes(pr, fill = cl)) + geom_density(adjust = 2, alpha = 0.5) + xlab("predicted probability")
# => Calculate the discrimination slope for both models. Ypu may want to consult the slides of the presentation to recall what that is. Which model is better in its discrimination slope?
mean(mitoses.test.pred.class[mitoses.test.pred.class$cl == '1', 'pr'])
mean(mitoses.test.pred.class[mitoses.test.pred.class$cl == '0', 'pr'])
mean(mitoses.thick.test.pred.class[mitoses.test.pred.class$cl == '1', 'pr'])
mean(mitoses.thick.test.pred.class[mitoses.test.pred.class$cl == '0', 'pr'])
# 0.4996795
# 0.2857597
# 0.78759
# 0.1593519
# Calculate the unsharpness of both models. Look at the slides in the presentation. Which model is sharpner (i.e. less unsharpness)?
unsharpness.mitoses.test <- sum(((mitoses.test.pred.class$pr * (1 - mitoses.test.pred.class$pr))) / nrow(mitoses.test.pred.class))
unsharpness.mitoses.thickness.test <- sum(((mitoses.thick.test.pred.class$pr * (1 - mitoses.thick.test.pred.class$pr))) / nrow(mitoses.thick.test.pred.class))
# unsharpness.mitoses.test = 0.18
# unsharpness.mitoses.thickness.test  = 0.10
pred.mitoses.test <- prediction(predicted.mitoses.test, test$Class) # Specify the predictions and observed outcome
perf.mitoses.test <- performance(pred.mitoses.test,"tpr","fpr") # Specify what to calculate, in our case tpr and fpr.
plot(perf.mitoses.test, colorize=F, col="green")
abline(0, 1)
pred.mitoses.thickness.test <- prediction(predicted.mitoses.thickness.test, test$Class)
perf.mitoses.thickness.test <- performance(pred.mitoses.thickness.test,"tpr","fpr")
plot(perf.mitoses.thickness.test, add=T, colorize=F, col="red") # Note the "add=T"  in order to plot on a pre existing plot
# Calculate the AUC for both models according to the "social party" we discussed in class (the proportion of times from all pairs in which the person with the event got higher probability of the event than the person without the event). Verify that you get the same results
auc_ROCR <- performance(pred.mitoses.test, measure = "auc")
auc_ROCR <- auc_ROCR@y.values[[1]]
auc_ROCR2 <- performance(pred.mitoses.thickness.test, measure = "auc")
auc_ROCR2 <- auc_ROCR2@y.values[[1]]
table(Actualvalue=test$Class,Predictedvalue=predicted.mitoses.test>0.50)
table(Actualvalue=test$Class,Predictedvalue=predicted.mitoses.thickness.test>0.50)
# AUC = 0.68
# AUC2 = 0.96
# => Calculate the NRI for those with malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm. You need to know how many times their probability improved (got higher) with the  mitoses.thickness.train.lrm model, and how many times it worsened. The difference between these two is the net improvement. You can then divide this difference by the number of patients with malignant breast cancer to obtain the proportion. This proportion is the NRI for those in class = 1.
# specify cutoff values for risk categories
# => Calculate the NRI for those with NO malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm.
cutoff <- c(0,0.51,1)
reclassification(data=test, cOutcome=10,predrisk1=predicted.mitoses.test, predrisk2=predicted.mitoses.thickness.test,cutoff)
# => Calculate the NRI for those with malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm. You need to know how many times their probability improved (got higher) with the  mitoses.thickness.train.lrm model, and how many times it worsened. The difference between these two is the net improvement. You can then divide this difference by the number of patients with malignant breast cancer to obtain the proportion. This proportion is the NRI for those in class = 1.
# specify cutoff values for risk categories
# => Calculate the NRI for those with NO malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm.
library("PredictABLE")
library(PredictABEL)
# => Calculate the NRI for those with malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm. You need to know how many times their probability improved (got higher) with the  mitoses.thickness.train.lrm model, and how many times it worsened. The difference between these two is the net improvement. You can then divide this difference by the number of patients with malignant breast cancer to obtain the proportion. This proportion is the NRI for those in class = 1.
# specify cutoff values for risk categories
# => Calculate the NRI for those with NO malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm.
#library("PredictABLE")
cutoff <- c(0,0.51,1)
reclassification(data=test, cOutcome=10,predrisk1=predicted.mitoses.test, predrisk2=predicted.mitoses.thickness.test,cutoff)
setwd("~/MAM_MIK/MAM_03")
library(readr)
costefficacydata <- read_csv("costefficacydata.csv")
View(costefficacydata)
data <- read_csv("costefficacydata.csv")
hund_icers <- calculate_icers(data$costs, data$trt)
install.packages("dampack")
library("dampack")
hund_icers <- calculate_icers(data$costs, data$trt)
hund_icers <- calculate_icers(data$costs, data$trt, NULL)
View(data)
hund_icers <- calculate_icers(data$costs, data$event, data$trt)
calculate_icers(data$costs, data$event, data$trt)
(mean_1 - mean_2)
table(data$trt)
surv_rate_1 <- length(which(data$trt == 1 & data$event == 1)) / (length(which(data$trt == 1))) # Survival rate group 1
surv_rate_2 <- length(which(data$trt == 2 & data$event == 1)) / (length(which(data$trt == 2))) # Survival rate group 2
n_1 <- length(which(data$trt == 1))
n_2 <- length(which(data$trt == 2))
diff_surv_rate <- surv_rate_1 - surv_rate_2 # Difference survival rate
log_RR <- log(surv_rate_1)-log(surv_rate_2) # LogRR
var_logRR <- (1 - surv_rate_1) / (n_1 * surv_rate_1) + (1 - surv_rate_2) / (n_2 * surv_rate_2)
root <- sqrt(var_logRR) #SE
log_RR_up <- log_RR + 1.96 * root #95% CI logRR
log_RR_low <- log_RR - 1.96 * root
log_CI_up <- exp(log_RR_up) #95 CI RR
log_CI_low <- exp(log_RR_low)
#####MEAN COSTS######
aggregate(data$costs, list(data$trt), FUN=mean) # Calculating the mean costs of both treatment groups
X<-split(data, data$trt)
tr_1 <- do.call(rbind.data.frame, X[1])
tr_2 <- do.call(rbind.data.frame, X[2])
t.test(tr_1$costs)$"conf.int"
t.test(tr_2$costs)$"conf.int"
t.test(data$costs)$"conf.int"
mean_1 <- mean(tr_1$costs)
mean_2 <- mean(tr_2$costs)
stde_costs_1 <- std.error(tr_1$costs)
stde_costs_2 <- std.error(tr_2$costs)
costs_1 <- c(tr_1$costs)
costs_2 <- c(tr_2$costs)
s_var_1 <- var(costs_1)
s_var_2 <- var(costs_2)
mean_diff <- mean_1 - mean_2
pooled_variance <- (((n_1-1)*s_var_1 + (n_2-1)*s_var_2)) / (n_1+n_2-2)
CI_costs_up = mean_diff + 2.06 * sqrt((pooled_variance/n_1)+(pooled_variance/n_2))
CI_costs_low = mean_diff - 2.06 * sqrt((pooled_variance/n_1)+(pooled_variance/n_2))
length(which(data$trt == 1 & data$event == 1))
var_surv1 <- 1 / (206 * surv_rate_1 * (1 - surv_rate_1))
var_surv2 <- 1 / (203 * surv_rate_2 * (1 - surv_rate_2))
length(which(data$trt == 2 & data$event == 1))
var_diff <- 1/52 + 1/154 + 1/74 + 1/129
var(costs_1)
var(costs_2)
(s_var_1 + s_var_2) / 409
var(costs_1) * ((206 - 1)/206)
s_var_1 <- var(costs_1) * ((206 - 1)/206)
s_var_2 <- var(costs_2) * ((203 - 1)/203)
(s_var_1 + s_var_2) / 409
var(tr_1$costs)
var(data$trt)
var(data$costs)
(s_var_1 + s_var_2) / 409
cov <- cov(data$trt, data$costs, method = "spearman")
2^2
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - vardiff)*(mean_diff - var_costs))^0.5
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5
var_costs <- (s_var_1 + s_var_2) / 409
cov <- cov(data$trt, data$costs, method = "spearman")
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5
mean_diff^2 - var_costs
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(mean_diff^2 - var_costs)
(diff_surv_rate * mean_diff) + ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(mean_diff^2 - var_costs)
ICER <- (mean_1 - mean_2) / (surv_rate_1 - surv_rate_2)
ICER <- (mean_diff) / (diff_surv_rate)
cov(data$trt, data$costs, method = "Pearson")
cov(data$trt, data$costs, method = "pearson")
cov <- cov(data$trt, data$costs, method = "pearson")
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(mean_diff^2 - var_costs)
(diff_surv_rate * mean_diff) + ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(mean_diff^2 - var_costs)
cov(data$trt, data$costs, method = "earson")
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
(diff_surv_rate * mean_diff) + ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_ratef^2 - var_diff)
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
(diff_surv_rate * mean_diff) + ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
diff_surv_rate
var_diff
stde_costs_1 <- std.error(tr_1$costs)
stde_costs_2 <- std.error(tr_2$costs)
library(statmod)
detach("package:stats", unload = TRUE)
library(stats)
std.error(tr_2$costs)
library(plotrix)
std.error(tr_2$costs)
std.error(tr_1$costs)
stde_costs_1^2/206
stde_costs_1 <- std.error(tr_1$costs)
stde_costs_2 <- std.error(tr_2$costs)
stde_costs_1^2/206
s_var_1 <- stde_costs_1^2/206
s_var_2 <- stde_costs_2^2/203
(s_var_1 + s_var_2) / 409
var_costs <- (s_var_1 + s_var_2) / 409
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
(diff_surv_rate * mean_diff) + ((diff_surv_rate * mean_diff - cov^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - 5000^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - 50^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
covCE <- cov(data$event[data$trt==1], data$costs[data$trt==1])/n1 + cov(data$event[data$trt==2], data$costs[data$trt==2])/n2
covCE <- cov(data$event[data$trt==1], data$costs[data$trt==1])/n_1 + cov(data$event[data$trt==2], data$costs[data$trt==2])/n_2
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - covCE^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
(diff_surv_rate * mean_diff) + ((diff_surv_rate * mean_diff - covCE^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
ICER <- (mean_diff) / (diff_surv_rate)
p1 <- mean(data$event[data$trt==1])
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - covCE^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
(diff_surv_rate * mean_diff) + ((diff_surv_rate * mean_diff - covCE^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
varlogCE = varC/(CC) + vare/(ee) - 2covCE/(Ce)
varlogCE = varC/(CC) + vare/(ee) - 2*covCE/(Ce)
var(data$costs[data$trt==1])/n_1
s_var_1 <- var(data$costs[data$trt==1])/n_1
s_var_2 <- var(data$costs[data$trt==2])/n_2
(diff_surv_rate * mean_diff) - ((diff_surv_rate * mean_diff - covCE^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
(diff_surv_rate * mean_diff) + ((diff_surv_rate * mean_diff - covCE^2)^2 - (diff_surv_rate^2 - var_diff)*(mean_diff - var_costs))^0.5/(diff_surv_rate^2 - var_diff)
var(data$costs[data$trt==1])/n_1
log(C/e)
log(C/e) - 1.96sqrt(varlogCE)
log(C/e) + 1.96sqrt(varlogCE)
log(C/e)
log(C/e) - 1.96*sqrt(varlogCE)
log(C/e) + 1.96*sqrt(varlogCE)
p1 <- mean(data$event[data$trt==1])
varp1 <- p1(1-p1)/n_1
p2 <- mean(data$event[data$trt==2])
varp2 <- p2(1-p2)/n_2
p1 <- mean(data$event[data$trt==1])
varp1 <- p1(1-p1)/n_1
p2 <- mean(data$event[data$trt==2])
varp2 <- p2(1-p2)/n_2
p1 <- mean(data$event[data$trt==1])
varp1 <- p1(1-p1)/n_1
p2 <- mean(data$event[data$trt==2])
varp2 <- p2(1-p2)/n_2
e <- p1-p2
vare <- varp1 + varp2
p1 <- mean(data$event[data$trt==1])
varp1 <- p1(1-p1)/n_1
p1 <- mean(data$event[data$trt==1])
varp1 <- p1*(1-p1)/n_1
p2 <- mean(data$event[data$trt==2])
varp2 <- p2*(1-p2)/n_2
e <- p1-p2
vare <- varp1 + varp2
varlogCE = varC/(CC) + vare/(ee) - 2*covCE/(Ce)
log(C/e)
log(C/e) - 1.96*sqrt(varlogCE)
log(C/e) + 1.96*sqrt(varlogCE)
log(mean_diff/e)
log(mean_diff/e) - 1.96*sqrt(varlogCE)
log(mean_diff/e) + 1.96*sqrt(varlogCE)
varlogCE = varC/(CC) + vare/(ee) - 2*covCE/(Ce)
var_costs <- s_var_1 - s_var_2
varlogCE = varC/(CC) + vare/(ee) - 2*covCE/(Ce)
varlogCE = var_costs/(CC) + vare/(ee) - 2*covCE/(Ce)
arlogCE = var_costs/(C*C) + vare/(e*e) - 2*covCE/(Ce)
varlogCE = var_costs/(mean_diff*mean_diff) + vare/(e*e) - 2*covCE/(Ce)
varlogCE = var_costs/(mean_diff*mean_diff) + vare/(e*e) - 2*covCE/(C*e)
varlogCE = var_costs/(mean_diff*mean_diff) + vare/(e*e) - 2*covCE/(mean_diff*e)
varlogCE <- var_costs/(mean_diff*mean_diff) + vare/(e*e) - 2*covCE/(mean_diff*e)
log(mean_diff/e)
log(mean_diff/e) - 1.96*sqrt(varlogCE)
log(mean_diff/e) + 1.96*sqrt(varlogCE)
