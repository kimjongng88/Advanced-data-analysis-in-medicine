plot(rf.roc)
rf.roc
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
table(y_pred, test$Class)
varImpPlot(rf.fit)
summary(y_pred)
summary(y_pred)
y_pred = predict(rf.fit, newdata = raw.test[,-1])
summary(y_pred)
summary(as_numeric(y_pred))
class(y_pred)
summary(y_pred)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = predict(rf.fit, newdata = test[,-10])
summary(y_pred)
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
table(y_pred, ValidSet$Condition)
table(y_pred, test$Class)
View(test)
y_pred
mean(y_pred == test$Class)
mean(y_pred == train$Class)
importance(rf.fit)
varImpPlot(rf.fit)
varImpPlot(rf.fit)
rf.fit <- randomForest(cc ~ ., data = raw.training, importance = T, type = "classification")
y_pred = predict(rf.fit, newdata = raw.test[,-1])
summary(y_pred)
rf.roc<-roc(raw.test$cc, as.numeric(y_pred))
plot(rf.roc)
rf.roc
View(boot.m)
test[,-10]
y_pred
y_pred = predict(rf.fit, newdata = test[,-10])
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = predict(rf.fit, newdata = test[,-10])
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = trunc(predict(rf.fit, newdata = test[,-10]))
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = predict(rf.fit, newdata = test[,-10])
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
y_pred = predict(rf.fit, newdata = test[,-10])
y_pred
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
table(y_pred, test$class)
table(y_pred, test$Class)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
summary(y_pred)
y_pred
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
table(y_pred, test$Class)
# Area under the curve: 0.9964. We controlled the complexity by limiting the number of trees permitted.
summary(y_pred)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
summary(y_pred)
y_pred
rf.fit <- randomForest(cc ~ ., data = raw.training, importance = T, type = "classification")
y_pred = predict(rf.fit, newdata = raw.test[,-1])
summary(y_pred)
rf.roc<-roc(raw.test$cc, as.numeric(y_pred))
plot(rf.roc)
rf.roc
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
summary(y_pred)
rf.roc<-roc(test$Class, y_pred)
plot(rf.roc)
rf.roc
table(y_pred, test$Class)
# Area under the curve: 0.9815. We controlled the complexity by limiting the number of trees permitted.
importance(y_pred)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
importance(y_pred)
importance(rf.fit)
rf.fit <- randomForest(Class ~ ., data = train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit <- randomForest(Class ~ ., data = train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit <- randomForest(Class ~ train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
rf.fit <- randomForest(data$Class ~ train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
rf.fit <- randomForest(train$Class ~ train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
rf.fit <- randomForest(train$Class ~ train$Cl.thickness + train$Cell.shape + train$Marg.adhesion + train$Bare.nuclei + train$Bl.cromatin + train$Normal.nucleoli + train$Cell.size + train$Epith.c.size, importance = T, type = "classification", ntrees = 500)
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
library("pROC")
library("e1071")
library("randomForest")
rf.fit <- randomForest(Class ~ ., data = train, importance = T, type = "classification", ntrees = 500)
importance(rf.fit)
varImpPlot(rf.fit)
y_pred = round(predict(rf.fit, newdata = test[,-10]))
y_pred
importance(rf.fit) # Determine importance variables
summary(y_pred)
rf.roc<-roc(test$Class, y_pred) # Compute RO-curve
plot(rf.roc) # Plot the cuve
rf.roc
table(y_pred, test$Class)
# Area under the curve: 0.9815. We controlled the complexity by limiting the number of trees permitted.
# When looking at the importance of the variables we have looked at the %IncMSE. By looking at these percentages we determine all variables are important, even though some more then others.
pROC::roc.test(rf.roc, AUC.ORIGINAL - mean(AUCS.m - AUCS.orig), method = "bootstrap", boot.n = 500, progress = "none", paired = T)
rf.roc
rf.roc$auc
pROC::roc.test(rf.roc$auc, AUC.ORIGINAL - mean(AUCS.m - AUCS.orig), method = "bootstrap", boot.n = 500, progress = "none", paired = T)
pROC::roc.test(rf.roc$auc, AUC.ORIGINAL, method = "bootstrap", boot.n = 500, progress = "none", paired = T)
d<-rf.roc$auc
pROC::roc.test(d, AUC.ORIGINAL, method = "bootstrap", boot.n = 500, progress = "none", paired = T)
pROC::roc.test(d, AUC.ORIGINAL,  boot.n = 500, progress = "none", paired = T)
pROC::roc.test(d, AUC.ORIGINAL, method = "delong",  boot.n = 500, progress = "none", paired = T)
library(tidyverse) # Includes many libraries like dplyr (for easy data maniputaion, readr (for reading datasets), ggplot2 (for creating elegant data visualisations), tibble (for easily working with data frames), etc.)
library(rms)       # for implementing Regression Modeling Strategies
library(ROCR)      # for visualizing the performance of prediction models
library(pROC)      # for Analyzing ROC curves
library(mlbench)   # for provision of machine learning benchmark problems
library(MASS)      # for stepwise variable selection
options(dplyr.print_min = 5L, dplyr.print_max = 5L) # set number of rows to show in a data frame
data(BreastCancer, package="mlbench")
names(BreastCancer) # * What are the names of the data frame bc?
nrow(BreastCancer)  # * How many obseravtion (rows) are there?
# Take a moment to look at the description of the database on http://ugrad.stat.ubc.ca/R/library/mlbench/html/BreastCancer.html
# Print the dataset
BreastCancer
summary(BreastCancer)
sum(is.na(BreastCancer$Bare.nuclei)) ## There are 16 missing values in the Bare.nuclei column.
# => Are there any missing values? There are 16 missing values in the Bare.nuclei column.
# => Write the code to calculate the number of missing values in each column. You can use whatever suits you. For example you can use "sapply" on BreastCancer to work on all columns while using the is.na function, or (preferably) you can use the power of dplyr commands like "summarise" together with sum and "is.na".
#
#
# => We will use only complete cases without any missing values in this excercise. Obtain the data frame "bc" with complete cases. Hint: look at the function complete.cases(). You can use it for example with the filter command of dplyr (or just use base R)
bc <- subset(BreastCancer, is.na(Bare.nuclei) == F)  # dataset with complete cases
# => How many obseravtion (rows) are there now?
# There are now 683 observations, because the 16 rows with missin g values in Bare.nuclei where deleted.
# How many cases did we remove?
# 16
# remove id column
bc <- subset(bc, select = -1) #=> Remove the "id" column from bc. Note that if you consider to use the "select" command in dplyr then this command may clash with the select command in the MASS library. Therefore use dplyr::select in that case.
# => convert the first 9 factors to numeric. You can use a for loop on these variables, or use "mutate_at".
bc$Cl.thickness <- as.numeric(bc$Cl.thickness)
bc$Cell.size <- as.numeric(bc$Cell.size)
bc$Cell.shape <- as.numeric(bc$Cell.shape)
bc$Marg.adhesion <- as.numeric(bc$Marg.adhesion)
bc$Epith.c.size <- as.numeric(bc$Epith.c.size)
bc$Bare.nuclei <-  as.numeric(bc$Bare.nuclei)
bc$Bl.cromatin <- as.numeric(bc$Bl.cromatin)
bc$Normal.nucleoli <- as.numeric(bc$Normal.nucleoli)
bc$Mitoses <- as.numeric(bc$Mitoses)
str(bc)
# Look at the class variable
bc$Class
# => change "malignant" into the number 1, and "bening" into the number 0. You can use a simple ifelse, or use the "recode" command in dplkr.
bc$Class <- ifelse(bc$Class == "malignant", 1, 0)
ddist <- datadist(bc) # preparation: in package rms we need to define the data distribution of the variables first. So before any models are fitted, this command stores the distribution summaries (e.g. ranges) for all potential variables.
options(datadist='ddist') # This means that when we fit a model we will also store the distribution information with the model object as well
mitoses.lrm <- lrm(Class~Mitoses, x=T, y=T, data=bc) # x = T and y=Y mean that the object mitoses.lrm will not only inlcude the model but will also keep the data as well. This is useful when we want access to the data via the model object.
summary(mitoses.lrm)
mitoses.thickness.lrm <- lrm(Class ~ Mitoses + Cl.thickness, x=T, y=T, data=bc) # fit a model that includes Mitoses and Cl.thickness as covariates
summary(mitoses.thickness.lrm)
set.seed(1234) # we fix the seed so that results are the same for all students
train.smp.size <- round(0.7 * nrow(bc))
test.smp.size <- round(0.3 * nrow(bc))
####roandom columns
train.ind <- sample(seq_len(nrow(bc)), size = train.smp.size)
train <-  bc[train.ind, ] # => Obrain train set consisting of 70% of the data
test <-  bc[-train.ind, ] #=> Obtain test set consisting of the rest of observations
mitoses.train.lrm <- lrm(Class ~ Mitoses, x=T, y=T, data=train) # => fit lrm model on the training set using only Mitoses, use x=T and y=T
mitoses.thickness.train.lrm <- lrm(Class ~ Mitoses + Cl.thickness, x=T, y=T, data=train) # => fit lrm model on the training set using  Mitoses and Cl.thickness, use x=T and y=T
predicted.mitoses.test <- predict(mitoses.train.lrm, newdata = test, type = 'fitted') # => obtain the predicted probabilities of mitoses.train.lrm on the test set. Hint: use the "predict" function. Important note: make sure you use the right "type" in the command to get probabilities and not log odds.
predicted.mitoses.thickness.test <- predict(mitoses.thickness.train.lrm, test, type = "fitted") # => obtain the predicted probabilities of mitoses.thickness.train.lrm on the test set.Check that they indeed are between 0 and 1 and have no negative numbers etc.
# => plot histogram of predicted.mitoses.test
hist(predicted.mitoses.test)
#* => plot histogram of predicted.mitoses.thickness.test
hist(predicted.mitoses.thickness.test)
#* => Obtain range of predicted.mitoses.test
summary(predicted.mitoses.test)
#* => Obtain range of predicted.mitoses.thickness.test
summary(predicted.mitoses.thickness.test)
mitoses.test.pred.class <- data.frame(pr=predicted.mitoses.test, cl = as.factor(test$Class))
ggplot(mitoses.test.pred.class, aes(pr, fill = cl)) + geom_density(adjust = 2, alpha = 0.5) + xlab("predicted probability")
# => For this density plot, which of the following statements are true:
# For those without breast cancer the probabilities are concentrated below 0.4 True
# For those with breast cancer the probabilities are concentrated above 0.8 False
# For those without breast cancer the probabilities are very high True
# For those with breast cancer they are likely to have any probability. True
# => What do you think that the "adjust" above does. Try the value 1 instead of 2. What does alpha do? Try alpha = 1
# We see that the color changes.
# => plot the probability density graph as before but now for the predicted.mitoses.thickness.test
mitoses.thick.test.pred.class <- data.frame(pr=predicted.mitoses.thickness.test, cl = as.factor(test$Class))
ggplot(mitoses.thick.test.pred.class, aes(pr, fill = cl)) + geom_density(adjust = 2, alpha = 0.5) + xlab("predicted probability")
# => Calculate the discrimination slope for both models. Ypu may want to consult the slides of the presentation to recall what that is. Which model is better in its discrimination slope?
mean(mitoses.test.pred.class[mitoses.test.pred.class$cl == '1', 'pr'])
mean(mitoses.test.pred.class[mitoses.test.pred.class$cl == '0', 'pr'])
mean(mitoses.thick.test.pred.class[mitoses.test.pred.class$cl == '1', 'pr'])
mean(mitoses.thick.test.pred.class[mitoses.test.pred.class$cl == '0', 'pr'])
# 0.4996795
# 0.2857597
# 0.78759
# 0.1593519
# Calculate the unsharpness of both models. Look at the slides in the presentation. Which model is sharpner (i.e. less unsharpness)?
unsharpness.mitoses.test <- sum(((mitoses.test.pred.class$pr * (1 - mitoses.test.pred.class$pr))) / nrow(mitoses.test.pred.class))
unsharpness.mitoses.thickness.test <- sum(((mitoses.thick.test.pred.class$pr * (1 - mitoses.thick.test.pred.class$pr))) / nrow(mitoses.thick.test.pred.class))
# unsharpness.mitoses.test = 0.18
# unsharpness.mitoses.thickness.test  = 0.10
pred.mitoses.test <- prediction(predicted.mitoses.test, test$Class) # Specify the predictions and observed outcome
perf.mitoses.test <- performance(pred.mitoses.test,"tpr","fpr") # Specify what to calculate, in our case tpr and fpr.
plot(perf.mitoses.test, colorize=F, col="green")
abline(0, 1)
pred.mitoses.thickness.test <- prediction(predicted.mitoses.thickness.test, test$Class)
perf.mitoses.thickness.test <- performance(pred.mitoses.thickness.test,"tpr","fpr")
plot(perf.mitoses.thickness.test, add=T, colorize=F, col="red") # Note the "add=T"  in order to plot on a pre existing plot
# Calculate the AUC for both models according to the "social party" we discussed in class (the proportion of times from all pairs in which the person with the event got higher probability of the event than the person without the event). Verify that you get the same results
auc_ROCR <- performance(pred.mitoses.test, measure = "auc")
auc_ROCR <- auc_ROCR@y.values[[1]]
auc_ROCR2 <- performance(pred.mitoses.thickness.test, measure = "auc")
auc_ROCR2 <- auc_ROCR2@y.values[[1]]
table(Actualvalue=test$Class,Predictedvalue=predicted.mitoses.test>0.50)
table(Actualvalue=test$Class,Predictedvalue=predicted.mitoses.thickness.test>0.50)
# AUC = 0.68
# AUC2 = 0.96
# => Calculate the NRI for those with malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm. You need to know how many times their probability improved (got higher) with the  mitoses.thickness.train.lrm model, and how many times it worsened. The difference between these two is the net improvement. You can then divide this difference by the number of patients with malignant breast cancer to obtain the proportion. This proportion is the NRI for those in class = 1.
# specify cutoff values for risk categories
# => Calculate the NRI for those with NO malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm.
cutoff <- c(0,0.51,1)
reclassification(data=test, cOutcome=10,predrisk1=predicted.mitoses.test, predrisk2=predicted.mitoses.thickness.test,cutoff)
# => Calculate the NRI for those with malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm. You need to know how many times their probability improved (got higher) with the  mitoses.thickness.train.lrm model, and how many times it worsened. The difference between these two is the net improvement. You can then divide this difference by the number of patients with malignant breast cancer to obtain the proportion. This proportion is the NRI for those in class = 1.
# specify cutoff values for risk categories
# => Calculate the NRI for those with NO malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm.
library("PredictABLE")
library(PredictABEL)
# => Calculate the NRI for those with malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm. You need to know how many times their probability improved (got higher) with the  mitoses.thickness.train.lrm model, and how many times it worsened. The difference between these two is the net improvement. You can then divide this difference by the number of patients with malignant breast cancer to obtain the proportion. This proportion is the NRI for those in class = 1.
# specify cutoff values for risk categories
# => Calculate the NRI for those with NO malignant breast cancer when using mitoses.thickness.train.lrm compared to mitoses.train.lrm.
#library("PredictABLE")
cutoff <- c(0,0.51,1)
reclassification(data=test, cOutcome=10,predrisk1=predicted.mitoses.test, predrisk2=predicted.mitoses.thickness.test,cutoff)
setwd("~/MAM_MIK/MAM_03/Assignment 2")
library(haven)
library(table1)
library(MASS)
renalt <-  read_sav("renaltx.sav")
renalt <- na.omit(renalt)
########TABLES#######
pvalue <- function(x, ...) {
# Construct vectors of data y, and groups (strata) g
y <- unlist(x)
g <- factor(rep(1:length(x), times=sapply(x, length)))
if (is.numeric(y)) {
# For numeric variables, perform a standard 2-sample t-test
p <- t.test(y ~ g)$p.value
} else {
# For categorical variables, perform a chi-squared test of independence
p <- chisq.test(table(y, g))$p.value
}
# Format the p-value, using an HTML entity for the less-than sign.
# The initial empty string places the output on the line below the variable label.
c("", sub("<", "&lt;", format.pval(p, digits=3, eps=0.001)))
}
renalt$dgf = as.factor(as.character(renalt$dgf))
renalt$uprotein = as.factor(as.character(renalt$uprotein))
renalt$uprotein = as.numeric(renalt$uprotein)
table1(~dgf + acclft + aantalre + creat + predias + prac + uprotein + cregsh + gsurv|gstatus,data = renalt, overall = F, extra.col = list('P-value' = pvalue))
multiStatus <- glm(gstatus ~ acclft + dgf + aantalre + creat + predias + prac + uprotein + cregsh, data=renalt)
summary(multiStatus) # show results
multiSurv <- glm(gsurv ~ acclft + dgf + aantalre + creat + predias + prac + uprotein + cregsh, data=renalt)
summary(multiSurv) # show results
renalt$dgf = as.factor(as.character(renalt$dgf))
renalt$aantalre = as.factor(as.character(renalt$aantalre))
renalt$uprotein = as.factor(as.character(renalt$uprotein))
multiStatus <- glm(gstatus ~ acclft + dgf + aantalre + creat + predias + prac + uprotein + cregsh, data=renalt)
summary(multiStatus) # show results
multiSurv <- glm(gsurv ~ acclft + dgf + aantalre + creat + predias + prac + uprotein + cregsh, data=renalt)
summary(multiSurv) # show results
acclft <- summary(glm(gstatus ~ acclft, family = "binomial", data = renalt, na.action = na.omit))
acclft
dgf <- summary(glm(gstatus ~ dgf, family = "binomial", data = renalt, na.action = na.omit))
dgf
aantalre <- summary(glm(gstatus ~ aantalre, family = "binomial", data = renalt, na.action = na.omit))
aantalre
creat <- summary(glm(gstatus ~ creat, family = "binomial", data = renalt, na.action = na.omit))
creat
predias <- summary(glm(gstatus ~ predias, family = "binomial", data = renalt, na.action = na.omit))
predias
prac <- summary(glm(gstatus ~ prac, family = "binomial", data = renalt, na.action = na.omit))
prac
uprotein <- summary(glm(gstatus ~ uprotein, family = "binomial", data = renalt, na.action = na.omit))
uprotein
cregsh <- summary(glm(gstatus ~ cregsh, family = "binomial", data = renalt, na.action = na.omit))
cregsh
acclft_surv <- summary(lm(gsurv ~ acclft, data = renalt, na.action = na.omit))
acclft_surv
dgf_surv <- summary(lm(gsurv ~ dgf, data = renalt, na.action = na.omit))
dgf_surv
aantalre_surv <- summary(lm(gsurv ~ aantalre, data = renalt, na.action = na.omit))
aantalre_surv
creat_surv <- summary(lm(gsurv ~ creat,data = renalt, na.action = na.omit))
creat_surv
predias_surv <- summary(lm(gsurv ~ predias, data = renalt, na.action = na.omit))
predias_surv
prac_surv <- summary(lm(gsurv ~ prac,  data = renalt, na.action = na.omit))
prac_surv
uprotein_surv <- summary(lm(gsurv ~ uprotein, data = renalt, na.action = na.omit))
uprotein_surv
cregsh_surv <- summary(lm(gsurv ~ cregsh, data = renalt, na.action = na.omit))
cregsh_surv
acclft_surv <- summary(lm(gsurv ~ acclft, data = renalt, na.action = na.omit))
acclft_surv
dgf_surv <- summary(lm(gsurv ~ dgf, data = renalt, na.action = na.omit))
dgf_surv
aantalre_surv <- summary(lm(gsurv ~ aantalre, data = renalt, na.action = na.omit))
aantalre_surv
creat_surv <- summary(lm(gsurv ~ creat,data = renalt, na.action = na.omit))
creat_surv
predias_surv <- summary(lm(gsurv ~ predias, data = renalt, na.action = na.omit))
predias_surv
prac_surv <- summary(lm(gsurv ~ prac,  data = renalt, na.action = na.omit))
prac_surv
uprotein_surv <- summary(lm(gsurv ~ uprotein, data = renalt, na.action = na.omit))
uprotein_surv
cregsh_surv <- summary(lm(gsurv ~ cregsh, data = renalt, na.action = na.omit))
cregsh_surv
renalt %>%
dplyr::summarise(meanLE=mean(gsurv,na.rm=TRUE),
medLE=median(gsurv,na.rm=TRUE),
sd=sd(gsurv,na.rm=TRUE),
iqr=IQR(gsurv,na.rm=TRUE),
Q1=quantile(gsurv,probs=0.25,na.rm=TRUE),
Q3=quantile(gsurv,probs=0.75),
n=n())
dplyr::summarise(meanLE=mean(gsurv,na.rm=TRUE),
medLE=median(gsurv,na.rm=TRUE),
sd=sd(gsurv,na.rm=TRUE),
iqr=IQR(gsurv,na.rm=TRUE),
Q1=quantile(gsurv,probs=0.25,na.rm=TRUE),
Q3=quantile(gsurv,probs=0.75),
n=n())
library('dplyr')
renalt %>%
dplyr::summarise(meanLE=mean(gsurv,na.rm=TRUE),
medLE=median(gsurv,na.rm=TRUE),
sd=sd(gsurv,na.rm=TRUE),
iqr=IQR(gsurv,na.rm=TRUE),
Q1=quantile(gsurv,probs=0.25,na.rm=TRUE),
Q3=quantile(gsurv,probs=0.75),
n=n())
funModeling::profiling_num(renalt)
library('funModeling')
library(funModeling)
install.packages('funModeling')
library('funModeling')
renalt %>%
dplyr::summarise(meanLE=mean(gsurv,na.rm=TRUE),
medLE=median(gsurv,na.rm=TRUE),
sd=sd(gsurv,na.rm=TRUE),
iqr=IQR(gsurv,na.rm=TRUE),
Q1=quantile(gsurv,probs=0.25,na.rm=TRUE),
Q3=quantile(gsurv,probs=0.75),
n=n())
funModeling::profiling_num(renalt)
funModeling::plot_num(gapminder)
funModeling::plot_num(renalt)
renalt2 %>%
dplyr::summarise(meanLE=mean(gsurv,na.rm=TRUE),
medLE=median(gsurv,na.rm=TRUE),
sd=sd(gsurv,na.rm=TRUE),
iqr=IQR(gsurv,na.rm=TRUE),
Q1=quantile(gsurv,probs=0.25,na.rm=TRUE),
Q3=quantile(gsurv,probs=0.75),
n=n())
renalt %>%
dplyr::summarise(meanLE=mean(gsurv,na.rm=TRUE),
medLE=median(gsurv,na.rm=TRUE),
sd=sd(gsurv,na.rm=TRUE),
iqr=IQR(gsurv,na.rm=TRUE),
Q1=quantile(gsurv,probs=0.25,na.rm=TRUE),
Q3=quantile(gsurv,probs=0.75),
n=n())
varlist <- c("gsurv")
renalt %>%
select(-year, -country, -pop) %>%
skimr::skim_without_charts() %>%
skimr::yank("numeric") %>%
dplyr::select(-one_of(varlist))
library("skimr")
install.packages("skimr")
varlist <- c("gsurv")
renalt %>%
select(-year, -country, -pop) %>%
skimr::skim_without_charts() %>%
skimr::yank("numeric") %>%
dplyr::select(-one_of(varlist))
varlist <- c(renalt$gsurv)
renalt %>%
select(-year, -country, -pop) %>%
skimr::skim_without_charts() %>%
skimr::yank("numeric") %>%
dplyr::select(-one_of(varlist))
varlist <- c("n_missing","complete_rate")
renalt %>%
select(gsurv) %>%
skimr::skim_without_charts() %>%
skimr::yank("numeric") %>%
dplyr::select(-one_of(varlist))
